# Importation des bibliothÃ¨ques nÃ©cessaires
import logging
import traceback
from sklearn.metrics import accuracy_score, f1_score, recall_score

# ===== LOGGING ======
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# ===== FONCTION D'Ã‰VALUATION DES PRÃ‰DICTIONS ======
def evaluate_metrics(y_true, y_pred):
    accuracy = accuracy_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)
    recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)
    return accuracy, f1, recall

# ===== FONCTION DE PREDICTION ET D'Ã‰VALUATION ======
def evaluate_and_predict(best_models, x_test, y_test):
    try:
        # Dictionnaire
        result = {}
        
        for name, model in best_models.items():
            # PrÃ©diction
            y_pred = model.predict(x_test)
            logger.info("ðŸš€ PrÃ©dictions effectuÃ©es")

            # Ã‰valuation des mÃ©triques
            accuracy, f1, recall = evaluate_metrics(y_test, y_pred)
            logger.info("âœ… Ã‰valuation terminÃ©e")
            
            result[name] = {
                "model": model,
                "metrics": {
                    "accuracy": accuracy,
                    "f1_score": f1,
                    "recall": recall
                }
}
        logger.info("ðŸ“Š PrÃ©diction + Evaluation terminÃ©es")

        return result
    except Exception as e:
        logger.error("Erreur lors de l'Ã©valuation : %s", str(e))
        logger.debug("Traceback complet : n%s", traceback.format_exc())
        raise e